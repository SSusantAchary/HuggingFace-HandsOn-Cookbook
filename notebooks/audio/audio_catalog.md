# Audio Notebook Catalog

<div align="center">
  <img src="../../assets/anime_icon/audio_mods.png" alt="Audio modality catalog cover" width="360">
</div>

| Model | Use case | Deps | Hardware | RAM | Notes | Notebook | Code |
|---|---|---|---|---|---|---|---|
| [Whisper Tiny](https://huggingface.co/openai/whisper-tiny)<br><sub>openai/whisper-tiny</sub> | ASR on short clips (EN/multilingual) | transformers, torchaudio, soundfile | CPU/GPU/MLX | <4GB | Install ffmpeg; good CPU baseline | audio/audio_notebooks/audio-01-whisper-tiny.ipynb | <img src="https://user-images.githubusercontent.com/74038190/213844263-a8897a51-32f4-4b3b-b5c2-e1528b89f6f3.png" width="32" /> [Whisper tiny/base ASR (HF pipeline)](https://github.com/huggingface/notebooks/blob/main/examples/automatic_speech_recognition.ipynb) |
| [Wav2Vec2 Base 960h](https://huggingface.co/facebook/wav2vec2-base-960h)<br><sub>facebook/wav2vec2-base-960h</sub> | ASR baseline (LibriSpeech-style) | transformers, torchaudio | CPU/GPU/MLX | 4–8GB | Works on CPU; add CTC decoding note | audio/audio_notebooks/audio-02-wav2vec2-base-960h.ipynb | <img src="https://user-images.githubusercontent.com/74038190/213866269-5d00981c-7c98-46d7-8a8e-16f462f15227.gif" width="32" /> [Whisper fine-tuning (English subset)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/asr_fine_tuning_whisper.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/asr_fine_tuning_whisper.ipynb) |
| [Whisper Base](https://huggingface.co/openai/whisper-base)<br><sub>openai/whisper-base</sub> | Balanced ASR quality vs speed | transformers, torchaudio, soundfile | CPU/GPU/MLX | 4–8GB | Use beam size 1 on CPU; ffmpeg needed | audio/audio_notebooks/audio-03-whisper-base.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212257454-16e3712e-945a-4ca2-b238-408ad0bf87e6.gif" width="32" /> [wav2vec2 ASR (base-960h)](https://github.com/huggingface/notebooks/blob/main/examples/wav2vec2_asr.ipynb) |
| [Whisper Small](https://huggingface.co/openai/whisper-small)<br><sub>openai/whisper-small</sub> | Improved ASR accuracy | transformers, torchaudio, soundfile | CPU/GPU/MLX | 8–16GB | GPU recommended; add language settings | audio/audio_notebooks/audio-04-whisper-small.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212257472-08e52665-c503-4bd9-aa20-f5a4dae769b5.gif" width="32" /> [HuBERT audio classification (SUPERB)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification_superb.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification_superb.ipynb) |
| [Whisper Medium](https://huggingface.co/openai/whisper-medium)<br><sub>openai/whisper-medium</sub> | High accuracy ASR | transformers, torchaudio, soundfile | CPU/GPU | 16–32GB | Plan quantization; GPU strongly advised | audio/audio_notebooks/audio-05-whisper-medium.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212257468-1e9a91f1-b626-4baa-b15d-5c385dfa7ed2.gif" width="32" /> [Keyword spotting (Speech Commands)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/keyword_spotting.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/keyword_spotting.ipynb) |
| [Wav2Vec2 XLSR EN](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english)<br><sub>jonatasgrosman/wav2vec2-large-xlsr-53-english</sub> | Multilingual ASR fine-tune | transformers, torchaudio | CPU/GPU | 8–16GB | Large memory; add vocab mapping | audio/audio_notebooks/audio-06-wav2vec2-xlsr-en.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212257465-7ce8d493-cac5-494e-982a-5a9deb852c4b.gif" width="32" /> [Speaker verification (ECAPA-TDNN, SpeechBrain)](https://colab.research.google.com/github/speechbrain/speechbrain/blob/develop/recipes/VoxCeleb/SpeakerRec/SVECAPA.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/speechbrain/speechbrain/blob/develop/recipes/VoxCeleb/SpeakerRec/SVECAPA.ipynb) |
| [HuBERT Large](https://huggingface.co/facebook/hubert-large-ls960-ft)<br><sub>facebook/hubert-large-ls960-ft</sub> | Self-supervised ASR feature extractor | transformers, torchaudio | CPU/GPU | 8–16GB | Great for finetuning; MIT license | audio/audio_notebooks/audio-07-hubert-large.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212257463-4d082cb4-7483-4eaf-bc25-6dde2628aabd.gif" width="32" /> [Speaker diarization (pyannote)](https://colab.research.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/diarization_api.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/diarization_api.ipynb) |
| [WavLM Base Plus](https://huggingface.co/microsoft/wavlm-base-plus)<br><sub>microsoft/wavlm-base-plus</sub> | Speech enhancement and ASR | transformers, torchaudio | CPU/GPU | 4–8GB | Use speechbrain recipes; MIT license | audio/audio_notebooks/audio-08-wavlm-base-plus.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212257460-738ff738-247f-4445-a718-cdd0ca76e2db.gif" width="32" /> [Audio emotion recognition (SUPERB ER)](https://colab.research.google.com/github/superbbenchmark/superb/blob/master/notebook/SUPERB_ER_demo.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/superbbenchmark/superb/blob/master/notebook/SUPERB_ER_demo.ipynb) |
| [WavLM Large](https://huggingface.co/microsoft/wavlm-large)<br><sub>microsoft/wavlm-large</sub> | Speaker diarization features | transformers, torchaudio | CPU/GPU | 8–16GB | Heavy but accurate; add VAD tip | audio/audio_notebooks/audio-09-wavlm-large.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212257467-871d32b7-e401-42e8-a166-fcfd7baa4c6b.gif" width="32" /> [TTS (Coqui-TTS basic colab)](https://colab.research.google.com/github/coqui-ai/TTS/blob/dev/notebooks/TTS_inference_demo.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/coqui-ai/TTS/blob/dev/notebooks/TTS_inference_demo.ipynb) |
| [HuBERT Emotion](https://huggingface.co/superb/hubert-base-superb-er)<br><sub>superb/hubert-base-superb-er</sub> | Emotion classification | transformers, torchaudio | CPU/GPU | 4–8GB | Requires torchaudio>=2.1; check labels | audio/audio_notebooks/audio-10-hubert-emotion.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212281756-450d3ffa-9335-4b98-a965-db8a18fee927.gif" width="32" /> [Voice activity detection (pyannote VAD)](https://colab.research.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/pipeline_demo.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/pyannote/pyannote-audio/blob/develop/tutorials/pipeline_demo.ipynb) |
| [Wav2Vec2 Speaker ID](https://huggingface.co/superb/wav2vec2-base-superb-sid)<br><sub>superb/wav2vec2-base-superb-sid</sub> | Speaker identification | transformers, torchaudio | CPU/GPU | 4–8GB | Add enrollment pipeline; MIT | audio/audio_notebooks/audio-11-wav2vec2-speaker-id.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212280805-9bcb336b-8c55-46a8-abf8-ff286ab55472.gif" width="32" /> [WavLM ASR / embeddings demo](https://colab.research.google.com/github/microsoft/unilm/blob/master/wavlm/notebooks/WavLM_Demo.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/microsoft/unilm/blob/master/wavlm/notebooks/WavLM_Demo.ipynb) |
| [UrbanSound8K ECAPA](https://huggingface.co/speechbrain/urbansound8k_ecapa)<br><sub>speechbrain/urbansound8k_ecapa</sub> | Urban sound classification | speechbrain, torchaudio | CPU/GPU | 4–8GB | Install ffmpeg; add data download step | audio/audio_notebooks/audio-12-urbansound8k-ecapa.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212280823-79088828-a258-4a4d-8d6c-96315d5a07af.gif" width="32" /> [XLS-R multilingual ASR](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_XLSR_Wav2Vec2_on_Arabic_ASR_with_Common_Voice.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_XLSR_Wav2Vec2_on_Arabic_ASR_with_Common_Voice.ipynb) |
| [ECAPA VoxCeleb](https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb)<br><sub>speechbrain/spkrec-ecapa-voxceleb</sub> | Speaker verification embeddings | speechbrain, torchaudio | CPU/GPU | 4–8GB | Add score calibration tip | audio/audio_notebooks/audio-13-ecapa-voxceleb.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212281763-e6ecd7ef-c4aa-45b6-a97c-f33f6bb592bd.gif" width="32" /> [Audio tagging (UrbanSound8K with HF)](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/audio_classification_hf.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/audio_classification_hf.ipynb) |
| [Wav2Vec2 Emotion](https://huggingface.co/speechbrain/emotion-recognition-wav2vec2-IEMOCAP)<br><sub>speechbrain/emotion-recognition-wav2vec2-IEMOCAP</sub> | Emotion recognition pipeline | speechbrain, torchaudio | CPU/GPU | 8–16GB | GPU improves speed; MIT license | audio/audio_notebooks/audio-14-wav2vec2-emotion.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212281775-b468df30-4edc-4bf8-a4ee-f52e1aaddc86.gif" width="32" /> [Streaming ASR with transformers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/asr_streaming.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/asr_streaming.ipynb) |
| [Speech Commands CNN](https://huggingface.co/speechbrain/google-speech-commands-cnn)<br><sub>speechbrain/google-speech-commands-cnn</sub> | Keyword spotting quickstart | speechbrain, torchaudio | CPU/GPU/MLX | <4GB | Great edge baseline; add noise aug | audio/audio_notebooks/audio-15-speech-commands-cnn.ipynb | <img src="https://user-images.githubusercontent.com/74038190/212281780-0afd9616-8310-46e9-a898-c4f5269f1387.gif" width="32" /> [Textless NLP (HuBERT units) demo](https://colab.research.google.com/github/facebookresearch/textlesslib/blob/main/notebooks/demo.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/facebookresearch/textlesslib/blob/main/notebooks/demo.ipynb) |
| [MIMIC VoiceBank](https://huggingface.co/speechbrain/mtl-mimic-voicebank)<br><sub>speechbrain/mtl-mimic-voicebank</sub> | Speech enhancement baseline | speechbrain, torchaudio | CPU/GPU | 4–8GB | Needs noise dataset; MIT license | audio/audio_notebooks/audio-16-mimic-voicebank.ipynb | <img src="https://github.com/Anmol-Baranwal/Cool-GIFs-For-GitHub/assets/74038190/1a797f46-efe4-41e6-9e75-5303e1bbcbfa" width="32" /> [Music tagging with AST](https://colab.research.google.com/github/qiuqiangkong/audioset_tagging_cnn/blob/master/colab/ast_audioset_demo.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/qiuqiangkong/audioset_tagging_cnn/blob/master/colab/ast_audioset_demo.ipynb) |
| [SpeechT5 TTS](https://huggingface.co/microsoft/speecht5_tts)<br><sub>microsoft/speecht5_tts</sub> | Text-to-speech neural | transformers, torchaudio | CPU/GPU | 8–16GB | Requires HiFi-GAN vocoder; MIT license | audio/audio_notebooks/audio-17-speecht5-tts.ipynb | <img src="https://github.com/Anmol-Baranwal/Cool-GIFs-For-GitHub/assets/74038190/29fd6286-4e7b-4d6c-818f-c4765d5e39a9" width="32" /> [Silero VAD + ASR integration](https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples/silero_vad_colab.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples/silero_vad_colab.ipynb) |
| [XTTS v2](https://huggingface.co/coqui/XTTS-v2)<br><sub>coqui/XTTS-v2</sub> | Multi-speaker TTS | TTS, torch | CPU/GPU | 16–32GB | Check Coqui license; needs ffmpeg | audio/audio_notebooks/audio-18-xtts-v2.ipynb | <img src="https://github.com/Anmol-Baranwal/Cool-GIFs-For-GitHub/assets/74038190/67f477ed-6624-42da-99f0-1a7b1a16eecb" width="32" /> [Audio augmentation & features (librosa)](https://colab.research.google.com/github/musikalkemist/AudioSignalProcessingForML/blob/master/03-Audio-Data-Augmentation.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/musikalkemist/AudioSignalProcessingForML/blob/master/03-Audio-Data-Augmentation.ipynb) |
| [VITS LJSpeech](https://huggingface.co/espnet/kan-bayashi_ljspeech_vits)<br><sub>espnet/kan-bayashi_ljspeech_vits</sub> | Fast TTS baseline | espnet, torchaudio | CPU/GPU | 8–16GB | Warm up for better quality; MIT | audio/audio_notebooks/audio-19-vits-ljspeech.ipynb | <img src="https://github.com/Anmol-Baranwal/Cool-GIFs-For-GitHub/assets/74038190/3c16d4f2-b757-4c70-8f42-43d5dddd2c36" width="32" /> [torchaudio pipeline tutorial](https://colab.research.google.com/github/pytorch/tutorials/blob/main/beginner_source/audio_classifier_tutorial.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/pytorch/tutorials/blob/main/beginner_source/audio_classifier_tutorial.ipynb) |
| [MMS TTS EN](https://huggingface.co/facebook/mms-tts-eng)<br><sub>facebook/mms-tts-eng</sub> | Multilingual MMS TTS | transformers, torchaudio | CPU/GPU | 8–16GB | License review: MMS; add phoneme note | audio/audio_notebooks/audio-20-mms-tts-en.ipynb | <img src="https://github.com/Anmol-Baranwal/Cool-GIFs-For-GitHub/assets/74038190/3fb2cdf6-8920-462e-87a4-95af376418aa" width="32" /> [Audio to embeddings (CLAP/LAION)](https://colab.research.google.com/github/LAION-AI/CLAP/blob/main/notebooks/CLAP_demo.ipynb) [![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/LAION-AI/CLAP/blob/main/notebooks/CLAP_demo.ipynb) |
| [Whisper Large V3](https://huggingface.co/openai/whisper-large-v3)<br><sub>openai/whisper-large-v3</sub> | Flagship multilingual ASR | transformers, torchaudio, soundfile | GPU | 24–48GB | Requires segmented decoding + flash attention | audio/audio_notebooks/audio-21-whisper-large-v3.ipynb | <img src="https://github.com/Anmol-Baranwal/Cool-GIFs-For-GitHub/assets/74038190/de038172-e903-4951-926c-755878deb0b4" width="32" /> [Whisper large inference tips](https://huggingface.co/blog/whisper) |
| [Whisper Large V3 Turbo](https://huggingface.co/openai/whisper-large-v3-turbo)<br><sub>openai/whisper-large-v3-turbo</sub> | Low-latency Whisper for live captioning | transformers, torchaudio, soundfile | GPU | 12–24GB | Optimized for <1s latency; pair with quantized weights | audio/audio_notebooks/audio-22-whisper-large-v3-turbo.ipynb | <img src="https://github.com/Anmol-Baranwal/Cool-GIFs-For-GitHub/assets/74038190/398b19b1-9aae-4c1f-8bc0-d172a2c08d68" width="32" /> [HF Whisper streaming example](https://huggingface.co/blog/whisper-streaming) |
| [Pyannote Speaker Diarization](https://huggingface.co/pyannote/speaker-diarization-3.1)<br><sub>pyannote/speaker-diarization-3.1</sub> | Speaker diarization + diarization-aware ASR | pyannote.audio, torch | GPU | 12–24GB | Needs SAD + embedding checkpoints pulled via HF Hub | audio/audio_notebooks/audio-23-pyannote-diarization.ipynb | <img src="https://github.com/Anmol-Baranwal/Cool-GIFs-For-GitHub/assets/74038190/e0d299f2-767c-4c21-bd49-90f2a19f1a78" width="32" /> [Pyannote diarization tutorial](https://huggingface.co/pyannote) |
| [Pyannote Voice Activity](https://huggingface.co/pyannote/voice-activity-detection)<br><sub>pyannote/voice-activity-detection</sub> | Voice activity detection front-end | pyannote.audio, torch | CPU/GPU | 4–8GB | Great for gating streaming Whisper pipelines | audio/audio_notebooks/audio-24-pyannote-vad.ipynb | <img src="https://user-images.githubusercontent.com/74038190/235294002-8aafea24-3179-45af-91d9-412ad7ff5359.gif" width="32" /> [Pyannote VAD notebook](https://huggingface.co/pyannote/voice-activity-detection) |
| [MMS Forced Aligner](https://huggingface.co/MahmoudAshraf/mms-300m-1130-forced-aligner)<br><sub>MahmoudAshraf/mms-300m-1130-forced-aligner</sub> | Multilingual speech-text forced alignment | transformers, torchaudio | GPU | 12–24GB | Supports 100+ languages; pre-download MMS tokenizer weights | audio/audio_notebooks/audio-25-mms-300m-aligner.ipynb | <img src="https://user-images.githubusercontent.com/74038190/235294007-de441046-823e-4eff-89bf-d4df52858b65.gif" width="32" /> [Meta MMS forced alignment guide](https://huggingface.co/meta-llama) |

_Source of truth: `/meta/notebook_catalog.csv`._

## Task-Specific Audio Datasets

| Task | Dataset | Size | Link |
|---|---|---|---|
| Speech Enhancement | Voicebank-Demand | 28 speakers, 35 noise types | [Download](https://datashare.ed.ac.uk/handle/10283/2791) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://datashare.ed.ac.uk/handle/10283/2791) |
| Keyword Spotting | Google Speech Commands V2 | 105k utterances | [Download](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html) |
| Emotion Recognition | IEMOCAP | ~12 hours dyadic sessions | [Download](https://sail.usc.edu/iemocap/) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://sail.usc.edu/iemocap/) |
| Environmental Sound Classification | ESC-50 | 2,000 clips across 50 classes | [Download](https://github.com/karoldvl/ESC-50) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/karoldvl/ESC-50) |
| Music Emotion Recognition | Emotify | 400 tracks with affect annotations | [Download](https://zenodo.org/record/3514953) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://zenodo.org/record/3514953) |
| Music Recommendation | Million Song Dataset | 1M tracks metadata | [Download](http://millionsongdataset.com/) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](http://millionsongdataset.com/) |
| Music Captioning | MusicCaps | 5,521 music-text pairs | [Download](https://www.kaggle.com/datasets/googleai/musiccaps) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://www.kaggle.com/datasets/googleai/musiccaps) |
| Drum Transcription | Groove MIDI | 1,150 performances | [Download](https://magenta.tensorflow.org/datasets/groove) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://magenta.tensorflow.org/datasets/groove) |
| Speech Translation | CoVoST 2 | 2,900 hours, 21 languages | [Download](https://github.com/facebookresearch/covost) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/facebookresearch/covost) |
| Speech Translation | MuST-C | 3,600 hours, 14 language pairs | [Download](https://aclanthology.org/N19-1202.pdf) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://aclanthology.org/N19-1202.pdf) |
| Sound Event Detection | DCASE Challenge | Varies by year | [Download](http://dcase.community/challenge2024) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](http://dcase.community/challenge2024) |
| Source Separation | MUSDB18-HQ | 150 songs (unaligned) | [Download](https://sigsep.github.io/datasets/musdb.html) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://sigsep.github.io/datasets/musdb.html) |

## Audio Datasets

| Title | Full Name | Size | Link |
| -------- | -------- | -------- | -------- |
| CommonVoice 11 | CommonVoice: A Massively Multilingual Speech Corpus | 58,250 voices (2,508 hours) | [Download](https://voice.mozilla.org/en/datasets) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://voice.mozilla.org/en/datasets) |
| Libri-Light | Libri-Light: A Benchmark for ASR with Limited or No Supervision | 60,000 hours | [Download](https://ai.facebook.com/tools/libri-light) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://ai.facebook.com/tools/libri-light) |
| Wenetspeech | Wenetspeech: A 10000+ hours multi-domain Mandarin corpus for speech recognition | 10,000 hours | [Download](https://github.com/wenet-e2e/WenetSpeech) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/wenet-e2e/WenetSpeech) |
| Gigaspeech | Gigaspeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio | 50,000 hours | [Download](https://github.com/SpeechColab/GigaSpeech) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/SpeechColab/GigaSpeech) |
| MuST-C | MuST-C: a Multilingual Speech Translation Corpus | 3,600 hours | [Download](https://aclanthology.org/N19-1202.pdf) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://aclanthology.org/N19-1202.pdf) |
| VoxPopuli | VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation | 400k hours | [Download](https://github.com/facebookresearch/voxpopuli) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/facebookresearch/voxpopuli) |
| CoVoST | CoVoST: A Large-Scale Multilingual Speech-To-Text Translation Corpus | 2,280 hours | [Download](https://github.com/facebookresearch/covost) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/facebookresearch/covost) |
| CVSS | CVSS: A Massively Multilingual Speech-to-Speech Translation Corpus | 3,909 hours | [Download](https://github.com/google-research-datasets/cvss) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/google-research-datasets/cvss) |
| EMIME | The EMIME bilingual database | – | [Download](https://www.emime.org/participate/emime-bilingual-database.html) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://www.emime.org/participate/emime-bilingual-database.html) |
| Audiocaps | Audiocaps: Generating captions for audios in the wild | 46,000 audios | [Download](https://github.com/cdjkim/audiocaps) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/cdjkim/audiocaps) |
| Clotho | Clotho: An audio captioning dataset | 4,981 audios • 24,905 captions | [Download](https://zenodo.org/record/3490684) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://zenodo.org/record/3490684) |
| AudioSet | AudioSet: An ontology and human-labeled dataset for audio events | 5.8k hours | [Download](https://g.co/audioset) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://g.co/audioset) |
| EMOPIA | EMOPIA: A multi-modal pop piano dataset for emotion recognition and emotion-based music generation | 387 piano solo tracks | [Download](https://zenodo.org/record/5090631) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://zenodo.org/record/5090631) |
| MetaMIDI | Building the MetaMIDI Dataset: Linking Symbolic and Audio Musical Data | 436,631 MIDI files | [Download](#) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](#) |
| DALI2 | Creating DALI, a Large Dataset of Synchronized Audio, Lyrics, and Notes | 7,756 songs | [Download](https://github.com/gabolsgabs/DALI) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/gabolsgabs/DALI) |
| MillionMIDI | Million MIDI Dataset (MMD) | 100k songs | [Download](#) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](#) |
| VGGSound | VGGSound: A Large-Scale Audio-Visual Dataset | 200k videos | [Download](https://www.robots.ox.ac.uk/~vgg/data/vggsound/) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://www.robots.ox.ac.uk/~vgg/data/vggsound/) |
| FSD50K | FSD50K: An Open Dataset of Human-Labeled Sound Events | 51,197 sound clips | [Download](https://zenodo.org/record/4060432) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://zenodo.org/record/4060432) |
| Symphony | Symphony generation with permutation invariant language model | 46,359 MIDI files | [Download](https://symphonynet.github.io/) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://symphonynet.github.io/) |
| MusicCaps | MusicLM: Generating Music From Text | 5,521 music-text pairs | [Download](https://www.kaggle.com/datasets/googleai/musiccaps) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://www.kaggle.com/datasets/googleai/musiccaps) |
| Jamendo | The MTG-Jamendo dataset for automatic music tagging | 55,525 tracks | [Download](https://github.com/MTG/mtg-jamendo-dataset) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://github.com/MTG/mtg-jamendo-dataset) |
| JamendoMaxCaps | JamendoMaxCaps: A Large-Scale Music-caption Dataset with Imputed Metadata | 25,356 hours • 362,238 songs | [Download](https://huggingface.co/datasets/amaai-lab/JamendoMaxCaps) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://huggingface.co/datasets/amaai-lab/JamendoMaxCaps) |
| MusicBench | Mustango: Toward Controllable Text-to-Music Generation | 53,168 tracks | [Download](https://huggingface.co/datasets/amaai-lab/MusicBench) [![Origin](https://img.shields.io/badge/Origin-F56C2D?style=for-the-badge&logo=origin&logoColor=white)](https://huggingface.co/datasets/amaai-lab/MusicBench) |
